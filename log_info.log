[2023-06-26 10:22:39,411][main.py][line:87][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': '/data/pyj/feat/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'lamda': 1, 'seed': 9, 'test_bs': 10, 'smooth': 'slide', 'kappa': 7, 'ckpt_path': './ckpt/ucf__8636.pkl', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0005, 'dropout': 0.1, 'train_bs': 128, 'max_seqlen': 200, 'max_epoch': 50, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2023-06-26 10:22:41,974][main.py][line:113][INFO] total params:1.2091M
[2023-06-26 10:22:41,975][main.py][line:116][INFO] Training Mode
[2023-06-26 10:22:41,976][main.py][line:53][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)

[2023-06-26 10:22:41,976][main.py][line:54][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0005
    lr: 0.0005
    weight_decay: 0
)

[2023-06-26 10:23:04,467][main.py][line:57][INFO] Random initialize AUC:0.5444 FAR:0.67905
[2023-06-26 10:28:20,970][main.py][line:74][INFO] [Epoch:1/50]: loss1:0.3674 loss2:1.0960 | AUC:0.8476 FAR:0.00341
[2023-06-26 10:30:40,865][main.py][line:74][INFO] [Epoch:2/50]: loss1:0.1536 loss2:0.8745 | AUC:0.8541 FAR:0.00519
[2023-06-26 10:33:03,047][main.py][line:74][INFO] [Epoch:3/50]: loss1:0.0670 loss2:0.7795 | AUC:0.8590 FAR:0.00314
[2023-06-26 10:35:17,897][main.py][line:74][INFO] [Epoch:4/50]: loss1:0.0374 loss2:0.7048 | AUC:0.8545 FAR:0.00242
[2023-06-26 10:37:32,002][main.py][line:74][INFO] [Epoch:5/50]: loss1:0.0248 loss2:0.6399 | AUC:0.8560 FAR:0.00232
[2023-06-26 10:39:47,095][main.py][line:74][INFO] [Epoch:6/50]: loss1:0.0166 loss2:0.5759 | AUC:0.8636 FAR:0.00580
[2023-06-26 10:42:02,062][main.py][line:74][INFO] [Epoch:7/50]: loss1:0.0365 loss2:0.5461 | AUC:0.8545 FAR:0.00328
[2023-06-26 10:44:15,828][main.py][line:74][INFO] [Epoch:8/50]: loss1:0.0120 loss2:0.4721 | AUC:0.8556 FAR:0.00264
[2023-06-26 10:46:30,014][main.py][line:74][INFO] [Epoch:9/50]: loss1:0.0087 loss2:0.4146 | AUC:0.8574 FAR:0.00259
[2023-06-26 10:48:41,037][main.py][line:74][INFO] [Epoch:10/50]: loss1:0.0081 loss2:0.3630 | AUC:0.8525 FAR:0.00370
[2023-06-26 10:50:55,236][main.py][line:74][INFO] [Epoch:11/50]: loss1:0.0073 loss2:0.3101 | AUC:0.8548 FAR:0.00408
[2023-06-26 10:53:07,085][main.py][line:74][INFO] [Epoch:12/50]: loss1:0.0064 loss2:0.2642 | AUC:0.8562 FAR:0.00235
[2023-06-26 10:55:18,935][main.py][line:74][INFO] [Epoch:13/50]: loss1:0.0161 loss2:0.2371 | AUC:0.8499 FAR:0.00790
[2023-06-26 10:57:37,384][main.py][line:74][INFO] [Epoch:14/50]: loss1:0.0484 loss2:0.2859 | AUC:0.8452 FAR:0.00368
[2023-06-26 11:00:35,001][main.py][line:74][INFO] [Epoch:15/50]: loss1:0.0098 loss2:0.2012 | AUC:0.8551 FAR:0.00304
[2023-06-26 11:03:11,084][main.py][line:74][INFO] [Epoch:16/50]: loss1:0.0094 loss2:0.1679 | AUC:0.8519 FAR:0.00306
[2023-06-26 11:05:38,862][main.py][line:74][INFO] [Epoch:17/50]: loss1:0.0147 loss2:0.1615 | AUC:0.8490 FAR:0.00277
[2023-06-26 11:08:03,205][main.py][line:74][INFO] [Epoch:18/50]: loss1:0.0047 loss2:0.1213 | AUC:0.8487 FAR:0.00227
[2023-06-26 11:10:24,212][main.py][line:74][INFO] [Epoch:19/50]: loss1:0.0036 loss2:0.0989 | AUC:0.8450 FAR:0.00245
[2023-06-26 11:13:19,243][main.py][line:74][INFO] [Epoch:20/50]: loss1:0.0041 loss2:0.0860 | AUC:0.8418 FAR:0.00232
[2023-06-26 11:15:38,669][main.py][line:74][INFO] [Epoch:21/50]: loss1:0.0029 loss2:0.0712 | AUC:0.8417 FAR:0.00200
[2023-06-26 11:18:00,753][main.py][line:74][INFO] [Epoch:22/50]: loss1:0.0024 loss2:0.0606 | AUC:0.8462 FAR:0.00235
[2023-06-26 11:20:18,162][main.py][line:74][INFO] [Epoch:23/50]: loss1:0.0020 loss2:0.0525 | AUC:0.8444 FAR:0.00311
[2023-06-26 11:22:40,464][main.py][line:74][INFO] [Epoch:24/50]: loss1:0.0017 loss2:0.0447 | AUC:0.8455 FAR:0.00232
[2023-06-26 11:24:56,506][main.py][line:74][INFO] [Epoch:25/50]: loss1:0.0018 loss2:0.0381 | AUC:0.8451 FAR:0.00319
[2023-06-26 11:27:15,959][main.py][line:74][INFO] [Epoch:26/50]: loss1:0.0017 loss2:0.0350 | AUC:0.8471 FAR:0.00257
[2023-06-26 11:29:41,893][main.py][line:74][INFO] [Epoch:27/50]: loss1:0.0015 loss2:0.0301 | AUC:0.8467 FAR:0.00232
[2023-06-26 11:32:09,740][main.py][line:74][INFO] [Epoch:28/50]: loss1:0.0014 loss2:0.0269 | AUC:0.8492 FAR:0.00262
[2023-06-26 11:34:33,227][main.py][line:74][INFO] [Epoch:29/50]: loss1:0.0010 loss2:0.0233 | AUC:0.8457 FAR:0.00299
[2023-06-26 11:37:08,390][main.py][line:74][INFO] [Epoch:30/50]: loss1:0.0258 loss2:0.0429 | AUC:0.8526 FAR:0.00294
[2023-06-26 11:39:32,361][main.py][line:74][INFO] [Epoch:31/50]: loss1:0.0112 loss2:0.0495 | AUC:0.8467 FAR:0.00294
[2023-06-26 11:41:57,373][main.py][line:74][INFO] [Epoch:32/50]: loss1:0.0028 loss2:0.0290 | AUC:0.8500 FAR:0.00289
[2023-06-26 11:44:23,142][main.py][line:74][INFO] [Epoch:33/50]: loss1:0.0014 loss2:0.0220 | AUC:0.8508 FAR:0.00319
[2023-06-26 11:46:44,860][main.py][line:74][INFO] [Epoch:34/50]: loss1:0.0010 loss2:0.0189 | AUC:0.8506 FAR:0.00343
[2023-06-26 11:49:10,181][main.py][line:74][INFO] [Epoch:35/50]: loss1:0.0010 loss2:0.0170 | AUC:0.8502 FAR:0.00408
[2023-06-26 11:51:38,606][main.py][line:74][INFO] [Epoch:36/50]: loss1:0.0011 loss2:0.0158 | AUC:0.8518 FAR:0.00314
[2023-06-26 11:53:58,995][main.py][line:74][INFO] [Epoch:37/50]: loss1:0.0029 loss2:0.0195 | AUC:0.8469 FAR:0.00380
[2023-06-26 11:56:12,383][main.py][line:74][INFO] [Epoch:38/50]: loss1:0.0009 loss2:0.0137 | AUC:0.8522 FAR:0.00368
[2023-06-26 11:58:23,435][main.py][line:74][INFO] [Epoch:39/50]: loss1:0.0008 loss2:0.0128 | AUC:0.8518 FAR:0.00366
[2023-06-26 12:00:33,337][main.py][line:74][INFO] [Epoch:40/50]: loss1:0.0006 loss2:0.0113 | AUC:0.8514 FAR:0.00380
[2023-06-26 12:02:45,002][main.py][line:74][INFO] [Epoch:41/50]: loss1:0.0006 loss2:0.0108 | AUC:0.8512 FAR:0.00405
[2023-06-26 12:05:00,560][main.py][line:74][INFO] [Epoch:42/50]: loss1:0.0005 loss2:0.0098 | AUC:0.8499 FAR:0.00316
[2023-06-26 12:07:11,449][main.py][line:74][INFO] [Epoch:43/50]: loss1:0.0005 loss2:0.0097 | AUC:0.8504 FAR:0.00368
[2023-06-26 12:09:23,488][main.py][line:74][INFO] [Epoch:44/50]: loss1:0.0005 loss2:0.0094 | AUC:0.8504 FAR:0.00353
[2023-06-26 12:11:36,193][main.py][line:74][INFO] [Epoch:45/50]: loss1:0.0005 loss2:0.0087 | AUC:0.8506 FAR:0.00385
[2023-06-26 12:13:48,758][main.py][line:74][INFO] [Epoch:46/50]: loss1:0.0004 loss2:0.0086 | AUC:0.8507 FAR:0.00348
[2023-06-26 12:15:58,737][main.py][line:74][INFO] [Epoch:47/50]: loss1:0.0005 loss2:0.0078 | AUC:0.8506 FAR:0.00346
[2023-06-26 12:18:23,042][main.py][line:74][INFO] [Epoch:48/50]: loss1:0.0004 loss2:0.0080 | AUC:0.8501 FAR:0.00326
[2023-06-26 12:20:36,653][main.py][line:74][INFO] [Epoch:49/50]: loss1:0.0004 loss2:0.0074 | AUC:0.8503 FAR:0.00373
[2023-06-26 12:23:07,479][main.py][line:74][INFO] [Epoch:50/50]: loss1:0.0004 loss2:0.0078 | AUC:0.8499 FAR:0.00341
[2023-06-26 12:23:07,491][main.py][line:80][INFO] Training completes in 120m 3s | best AUC:0.8636 FAR:0.00580

[2023-06-26 12:58:17,606][main.py][line:87][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': '/data/pyj/feat/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'lamda': 1, 'seed': 9, 'test_bs': 10, 'smooth': 'slide', 'kappa': 7, 'ckpt_path': './ckpt/ucf__8636.pkl', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0005, 'dropout': 0.1, 'train_bs': 128, 'max_seqlen': 200, 'max_epoch': 50, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2023-06-26 12:58:19,910][main.py][line:113][INFO] total params:1.2091M
[2023-06-26 12:58:19,910][main.py][line:120][INFO] Test Mode
[2023-06-26 12:58:19,910][main.py][line:26][INFO] loading pretrained checkpoint from ./ckpt/ucf__8636.pkl.
[2023-06-26 12:58:32,117][infer.py][line:47][INFO] offline AUC:0.8676 AP:0.3399 FAR:0.0047 | Complete in 0m 12s

[2023-06-26 12:59:02,787][main.py][line:87][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': '/data/pyj/feat/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'lamda': 1, 'seed': 4, 'test_bs': 5, 'smooth': 'fixed', 'kappa': 8, 'ckpt_path': './ckpt/xd__8526.pkl', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0005, 'dropout': 0.1, 'train_bs': 128, 'max_seqlen': 200, 'max_epoch': 50, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2023-06-26 12:59:05,198][main.py][line:113][INFO] total params:1.2073M
[2023-06-26 12:59:05,198][main.py][line:116][INFO] Training Mode
[2023-06-26 12:59:05,199][main.py][line:53][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
)

[2023-06-26 12:59:05,200][main.py][line:54][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0005
    lr: 0.0005
    weight_decay: 0
)

[2023-06-26 12:59:18,902][main.py][line:57][INFO] Random initialize AP:0.2742 FAR:0.44700
[2023-06-26 13:01:57,379][main.py][line:74][INFO] [Epoch:1/50]: loss1:0.3384 loss2:0.5558 | AUC:0.7968 FAR:0.00456
[2023-06-26 13:04:52,989][main.py][line:74][INFO] [Epoch:2/50]: loss1:0.1947 loss2:0.3543 | AUC:0.8526 FAR:0.00625
[2023-06-26 13:07:34,256][main.py][line:74][INFO] [Epoch:3/50]: loss1:0.1335 loss2:0.2872 | AUC:0.8308 FAR:0.00221
[2023-06-26 13:10:18,471][main.py][line:74][INFO] [Epoch:4/50]: loss1:0.0996 loss2:0.2413 | AUC:0.8173 FAR:0.00126
[2023-06-26 13:12:57,282][main.py][line:74][INFO] [Epoch:5/50]: loss1:0.0694 loss2:0.2068 | AUC:0.8211 FAR:0.00284
[2023-06-26 13:15:36,887][main.py][line:74][INFO] [Epoch:6/50]: loss1:0.0557 loss2:0.1810 | AUC:0.7612 FAR:0.00112
[2023-06-26 13:18:18,239][main.py][line:74][INFO] [Epoch:7/50]: loss1:0.0558 loss2:0.1695 | AUC:0.8357 FAR:0.00404
[2023-06-26 13:20:57,396][main.py][line:74][INFO] [Epoch:8/50]: loss1:0.0400 loss2:0.1440 | AUC:0.8156 FAR:0.00363
[2023-06-26 13:23:37,743][main.py][line:74][INFO] [Epoch:9/50]: loss1:0.0278 loss2:0.1172 | AUC:0.7929 FAR:0.00229
[2023-06-26 13:26:19,042][main.py][line:74][INFO] [Epoch:10/50]: loss1:0.0448 loss2:0.1164 | AUC:0.8142 FAR:0.00328
[2023-06-26 13:28:58,999][main.py][line:74][INFO] [Epoch:11/50]: loss1:0.0307 loss2:0.1071 | AUC:0.8013 FAR:0.00066
[2023-06-26 13:31:42,415][main.py][line:74][INFO] [Epoch:12/50]: loss1:0.0209 loss2:0.0798 | AUC:0.8348 FAR:0.00309
[2023-06-26 13:34:19,254][main.py][line:74][INFO] [Epoch:13/50]: loss1:0.0175 loss2:0.0514 | AUC:0.8350 FAR:0.00263
[2023-06-26 13:36:51,850][main.py][line:74][INFO] [Epoch:14/50]: loss1:0.0101 loss2:0.0470 | AUC:0.7933 FAR:0.00080
[2023-06-26 13:39:23,674][main.py][line:74][INFO] [Epoch:15/50]: loss1:0.0108 loss2:0.0437 | AUC:0.8283 FAR:0.00231
[2023-06-26 13:42:20,757][main.py][line:74][INFO] [Epoch:16/50]: loss1:0.0082 loss2:0.0198 | AUC:0.7936 FAR:0.00064
[2023-06-26 13:44:59,435][main.py][line:74][INFO] [Epoch:17/50]: loss1:0.0154 loss2:0.0166 | AUC:0.8021 FAR:0.00121
[2023-06-26 13:47:50,154][main.py][line:74][INFO] [Epoch:18/50]: loss1:0.0185 loss2:0.0014 | AUC:0.8113 FAR:0.00184
[2023-06-26 13:50:30,184][main.py][line:74][INFO] [Epoch:19/50]: loss1:0.0046 loss2:0.0000 | AUC:0.8097 FAR:0.00215
[2023-06-26 13:53:36,445][main.py][line:74][INFO] [Epoch:20/50]: loss1:0.0036 loss2:0.0000 | AUC:0.8255 FAR:0.00179
[2023-06-26 13:56:21,920][main.py][line:74][INFO] [Epoch:21/50]: loss1:0.0255 loss2:0.0442 | AUC:0.7577 FAR:0.00151
[2023-06-26 13:58:58,932][main.py][line:74][INFO] [Epoch:22/50]: loss1:0.0109 loss2:0.0460 | AUC:0.7752 FAR:0.00054
[2023-06-26 14:01:49,618][main.py][line:74][INFO] [Epoch:23/50]: loss1:0.0079 loss2:0.0030 | AUC:0.8155 FAR:0.00156
[2023-06-26 14:04:31,046][main.py][line:74][INFO] [Epoch:24/50]: loss1:0.0082 loss2:0.0057 | AUC:0.8271 FAR:0.00207
[2023-06-26 14:07:17,468][main.py][line:74][INFO] [Epoch:25/50]: loss1:0.0022 loss2:0.0000 | AUC:0.8208 FAR:0.00239
[2023-06-26 14:10:09,596][main.py][line:74][INFO] [Epoch:26/50]: loss1:0.0061 loss2:0.0000 | AUC:0.8141 FAR:0.00144
[2023-06-26 14:13:11,678][main.py][line:74][INFO] [Epoch:27/50]: loss1:0.0033 loss2:0.0000 | AUC:0.8114 FAR:0.00133
[2023-06-26 14:15:41,063][main.py][line:74][INFO] [Epoch:28/50]: loss1:0.0025 loss2:0.0000 | AUC:0.8147 FAR:0.00180
[2023-06-26 14:18:21,049][main.py][line:74][INFO] [Epoch:29/50]: loss1:0.0018 loss2:0.0000 | AUC:0.8146 FAR:0.00144
[2023-06-26 14:21:01,533][main.py][line:74][INFO] [Epoch:30/50]: loss1:0.0010 loss2:0.0000 | AUC:0.8137 FAR:0.00170
[2023-06-26 14:23:50,767][main.py][line:74][INFO] [Epoch:31/50]: loss1:0.0116 loss2:0.0000 | AUC:0.8052 FAR:0.00183
[2023-06-26 14:26:37,664][main.py][line:74][INFO] [Epoch:32/50]: loss1:0.0133 loss2:0.0085 | AUC:0.8202 FAR:0.00126
[2023-06-26 14:29:21,201][main.py][line:74][INFO] [Epoch:33/50]: loss1:0.0022 loss2:0.0000 | AUC:0.8125 FAR:0.00354
[2023-06-26 14:32:20,565][main.py][line:74][INFO] [Epoch:34/50]: loss1:0.0011 loss2:0.0000 | AUC:0.8175 FAR:0.00199
[2023-06-26 14:35:02,375][main.py][line:74][INFO] [Epoch:35/50]: loss1:0.0018 loss2:0.0000 | AUC:0.8223 FAR:0.00252
[2023-06-26 14:37:52,408][main.py][line:74][INFO] [Epoch:36/50]: loss1:0.0009 loss2:0.0000 | AUC:0.8187 FAR:0.00286
[2023-06-26 14:40:35,544][main.py][line:74][INFO] [Epoch:37/50]: loss1:0.0007 loss2:0.0000 | AUC:0.8142 FAR:0.00266
[2023-06-26 14:43:26,219][main.py][line:74][INFO] [Epoch:38/50]: loss1:0.0006 loss2:0.0000 | AUC:0.8145 FAR:0.00217
[2023-06-26 14:46:16,282][main.py][line:74][INFO] [Epoch:39/50]: loss1:0.0006 loss2:0.0000 | AUC:0.8158 FAR:0.00236
[2023-06-26 14:49:05,279][main.py][line:74][INFO] [Epoch:40/50]: loss1:0.0006 loss2:0.0000 | AUC:0.8171 FAR:0.00236
[2023-06-26 14:51:52,563][main.py][line:74][INFO] [Epoch:41/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8156 FAR:0.00224
[2023-06-26 14:54:41,250][main.py][line:74][INFO] [Epoch:42/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8161 FAR:0.00212
[2023-06-26 14:57:32,924][main.py][line:74][INFO] [Epoch:43/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8148 FAR:0.00222
[2023-06-26 15:00:19,503][main.py][line:74][INFO] [Epoch:44/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8143 FAR:0.00239
[2023-06-26 15:03:07,044][main.py][line:74][INFO] [Epoch:45/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8150 FAR:0.00221
[2023-06-26 15:05:51,733][main.py][line:74][INFO] [Epoch:46/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8160 FAR:0.00236
[2023-06-26 15:08:43,028][main.py][line:74][INFO] [Epoch:47/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8150 FAR:0.00204
[2023-06-26 15:11:28,315][main.py][line:74][INFO] [Epoch:48/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8138 FAR:0.00220
[2023-06-26 15:14:14,464][main.py][line:74][INFO] [Epoch:49/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8136 FAR:0.00221
[2023-06-26 15:16:59,857][main.py][line:74][INFO] [Epoch:50/50]: loss1:0.0005 loss2:0.0000 | AUC:0.8146 FAR:0.00222
[2023-06-26 15:16:59,870][main.py][line:80][INFO] Training completes in 137m 41s | best AP:0.8526 FAR:0.00625

[2023-06-26 15:41:47,285][main.py][line:87][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': '/data/pyj/feat/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'lamda': 1, 'seed': 4, 'test_bs': 5, 'smooth': 'fixed', 'kappa': 8, 'ckpt_path': './ckpt/xd__8526.pkl', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0005, 'dropout': 0.1, 'train_bs': 128, 'max_seqlen': 200, 'max_epoch': 50, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2023-06-26 15:41:49,642][main.py][line:113][INFO] total params:1.2073M
[2023-06-26 15:41:49,643][main.py][line:120][INFO] Test Mode
[2023-06-26 15:41:49,643][main.py][line:26][INFO] loading pretrained checkpoint from ./ckpt/xd__8526.pkl.
[2023-06-26 15:42:03,950][infer.py][line:47][INFO] offline AUC:0.9494 AP:0.8559 FAR:0.0057 | Complete in 0m 14s

[2023-06-26 15:57:20,503][main.py][line:87][INFO] Config:{'dataset': 'shanghaiTech', 'model_name': 'SH_', 'metrics': 'AUC', 'feat_prefix': '/data/pyj/feat/SHTech-i3d', 'train_list': './list/sh/train.list', 'test_list': './list/sh/test.list', 'token_feat': './list/sh/sh-prompt.npy', 'abn_label': './list/sh/relabel.list', 'gt': './list/sh/sh-gt.npy', 'win_size': 5, 'gamma': 0.08, 'bias': 0.1, 'norm': True, 't_step': 3, 'temp': 0.2, 'lamda': 9, 'seed': 0, 'test_bs': 10, 'smooth': 'slide', 'kappa': 3, 'ckpt_path': './ckpt/SH__98.pkl', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0005, 'dropout': 0.1, 'train_bs': 128, 'max_seqlen': 200, 'max_epoch': 50, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2023-06-26 15:57:22,852][main.py][line:113][INFO] total params:1.2073M
[2023-06-26 15:57:22,852][main.py][line:116][INFO] Training Mode
[2023-06-26 15:57:22,853][main.py][line:53][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
)

[2023-06-26 15:57:22,853][main.py][line:54][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0005
    lr: 0.0005
    weight_decay: 0
)

[2023-06-26 15:57:28,770][main.py][line:57][INFO] Random initialize AUC:0.5546 FAR:0.00402
[2023-06-26 15:57:47,743][main.py][line:74][INFO] [Epoch:1/50]: loss1:0.4849 loss2:0.7652 | AUC:0.9503 FAR:0.00013
[2023-06-26 15:58:02,767][main.py][line:74][INFO] [Epoch:2/50]: loss1:0.2230 loss2:0.6820 | AUC:0.9743 FAR:0.00013
[2023-06-26 15:58:17,780][main.py][line:74][INFO] [Epoch:3/50]: loss1:0.1478 loss2:0.6645 | AUC:0.9720 FAR:0.00052
[2023-06-26 15:58:33,830][main.py][line:74][INFO] [Epoch:4/50]: loss1:0.1041 loss2:0.6497 | AUC:0.9708 FAR:0.00675
[2023-06-26 15:58:48,718][main.py][line:74][INFO] [Epoch:5/50]: loss1:0.0817 loss2:0.6001 | AUC:0.9727 FAR:0.00117
[2023-06-26 15:59:03,862][main.py][line:74][INFO] [Epoch:6/50]: loss1:0.0786 loss2:0.5964 | AUC:0.9701 FAR:0.00221
[2023-06-26 15:59:25,136][main.py][line:74][INFO] [Epoch:7/50]: loss1:0.0591 loss2:0.5651 | AUC:0.9710 FAR:0.00065
[2023-06-26 15:59:44,337][main.py][line:74][INFO] [Epoch:8/50]: loss1:0.0509 loss2:0.5515 | AUC:0.9725 FAR:0.00039
[2023-06-26 15:59:59,596][main.py][line:74][INFO] [Epoch:9/50]: loss1:0.0404 loss2:0.5450 | AUC:0.9747 FAR:0.00039
[2023-06-26 16:00:16,175][main.py][line:74][INFO] [Epoch:10/50]: loss1:0.0459 loss2:0.5249 | AUC:0.9747 FAR:0.00286
[2023-06-26 16:00:31,393][main.py][line:74][INFO] [Epoch:11/50]: loss1:0.0296 loss2:0.4967 | AUC:0.9746 FAR:0.00026
[2023-06-26 16:00:46,321][main.py][line:74][INFO] [Epoch:12/50]: loss1:0.0291 loss2:0.4867 | AUC:0.9745 FAR:0.00208
[2023-06-26 16:01:01,580][main.py][line:74][INFO] [Epoch:13/50]: loss1:0.0219 loss2:0.4663 | AUC:0.9758 FAR:0.00234
[2023-06-26 16:01:16,639][main.py][line:74][INFO] [Epoch:14/50]: loss1:0.0229 loss2:0.4645 | AUC:0.9757 FAR:0.00013
[2023-06-26 16:01:31,589][main.py][line:74][INFO] [Epoch:15/50]: loss1:0.0174 loss2:0.4635 | AUC:0.9765 FAR:0.00026
[2023-06-26 16:01:46,888][main.py][line:74][INFO] [Epoch:16/50]: loss1:0.0178 loss2:0.4456 | AUC:0.9756 FAR:0.00104
[2023-06-26 16:02:03,523][main.py][line:74][INFO] [Epoch:17/50]: loss1:0.0147 loss2:0.4356 | AUC:0.9780 FAR:0.00039
[2023-06-26 16:02:18,675][main.py][line:74][INFO] [Epoch:18/50]: loss1:0.0140 loss2:0.4257 | AUC:0.9786 FAR:0.00000
[2023-06-26 16:02:33,844][main.py][line:74][INFO] [Epoch:19/50]: loss1:0.0131 loss2:0.4216 | AUC:0.9764 FAR:0.00026
[2023-06-26 16:02:49,216][main.py][line:74][INFO] [Epoch:20/50]: loss1:0.0161 loss2:0.4072 | AUC:0.9791 FAR:0.00013
[2023-06-26 16:03:04,618][main.py][line:74][INFO] [Epoch:21/50]: loss1:0.0127 loss2:0.3953 | AUC:0.9773 FAR:0.00026
[2023-06-26 16:03:20,056][main.py][line:74][INFO] [Epoch:22/50]: loss1:0.0160 loss2:0.4061 | AUC:0.9800 FAR:0.00000
[2023-06-26 16:03:35,280][main.py][line:74][INFO] [Epoch:23/50]: loss1:0.0083 loss2:0.3886 | AUC:0.9787 FAR:0.00052
[2023-06-26 16:03:50,522][main.py][line:74][INFO] [Epoch:24/50]: loss1:0.0090 loss2:0.3822 | AUC:0.9796 FAR:0.00091
[2023-06-26 16:04:05,581][main.py][line:74][INFO] [Epoch:25/50]: loss1:0.0085 loss2:0.3805 | AUC:0.9793 FAR:0.00052
[2023-06-26 16:04:20,777][main.py][line:74][INFO] [Epoch:26/50]: loss1:0.0087 loss2:0.3793 | AUC:0.9785 FAR:0.00026
[2023-06-26 16:04:36,022][main.py][line:74][INFO] [Epoch:27/50]: loss1:0.0082 loss2:0.3715 | AUC:0.9787 FAR:0.00117
[2023-06-26 16:04:51,156][main.py][line:74][INFO] [Epoch:28/50]: loss1:0.0069 loss2:0.3495 | AUC:0.9791 FAR:0.00026
[2023-06-26 16:05:06,556][main.py][line:74][INFO] [Epoch:29/50]: loss1:0.0068 loss2:0.3575 | AUC:0.9794 FAR:0.00026
[2023-06-26 16:05:21,932][main.py][line:74][INFO] [Epoch:30/50]: loss1:0.0066 loss2:0.3542 | AUC:0.9788 FAR:0.00052
[2023-06-26 16:05:37,223][main.py][line:74][INFO] [Epoch:31/50]: loss1:0.0071 loss2:0.3470 | AUC:0.9789 FAR:0.00039
[2023-06-26 16:05:52,259][main.py][line:74][INFO] [Epoch:32/50]: loss1:0.0077 loss2:0.3470 | AUC:0.9764 FAR:0.00026
[2023-06-26 16:06:07,467][main.py][line:74][INFO] [Epoch:33/50]: loss1:0.0122 loss2:0.3454 | AUC:0.9787 FAR:0.00052
[2023-06-26 16:06:26,627][main.py][line:74][INFO] [Epoch:34/50]: loss1:0.0067 loss2:0.3325 | AUC:0.9787 FAR:0.00026
[2023-06-26 16:06:41,889][main.py][line:74][INFO] [Epoch:35/50]: loss1:0.0075 loss2:0.3349 | AUC:0.9791 FAR:0.00078
[2023-06-26 16:06:57,134][main.py][line:74][INFO] [Epoch:36/50]: loss1:0.0073 loss2:0.3387 | AUC:0.9782 FAR:0.00026
[2023-06-26 16:07:12,229][main.py][line:74][INFO] [Epoch:37/50]: loss1:0.0064 loss2:0.3327 | AUC:0.9781 FAR:0.00039
[2023-06-26 16:07:27,618][main.py][line:74][INFO] [Epoch:38/50]: loss1:0.0057 loss2:0.3290 | AUC:0.9792 FAR:0.00026
[2023-06-26 16:07:42,658][main.py][line:74][INFO] [Epoch:39/50]: loss1:0.0060 loss2:0.3263 | AUC:0.9782 FAR:0.00026
[2023-06-26 16:07:57,784][main.py][line:74][INFO] [Epoch:40/50]: loss1:0.0056 loss2:0.3296 | AUC:0.9785 FAR:0.00026
[2023-06-26 16:08:12,894][main.py][line:74][INFO] [Epoch:41/50]: loss1:0.0053 loss2:0.3223 | AUC:0.9791 FAR:0.00026
[2023-06-26 16:08:28,308][main.py][line:74][INFO] [Epoch:42/50]: loss1:0.0051 loss2:0.3198 | AUC:0.9784 FAR:0.00026
[2023-06-26 16:08:43,714][main.py][line:74][INFO] [Epoch:43/50]: loss1:0.0047 loss2:0.3193 | AUC:0.9782 FAR:0.00026
[2023-06-26 16:08:59,230][main.py][line:74][INFO] [Epoch:44/50]: loss1:0.0052 loss2:0.3182 | AUC:0.9784 FAR:0.00013
[2023-06-26 16:09:14,430][main.py][line:74][INFO] [Epoch:45/50]: loss1:0.0055 loss2:0.3144 | AUC:0.9780 FAR:0.00039
[2023-06-26 16:09:29,465][main.py][line:74][INFO] [Epoch:46/50]: loss1:0.0051 loss2:0.3161 | AUC:0.9782 FAR:0.00052
[2023-06-26 16:09:44,569][main.py][line:74][INFO] [Epoch:47/50]: loss1:0.0054 loss2:0.3129 | AUC:0.9780 FAR:0.00026
[2023-06-26 16:09:59,630][main.py][line:74][INFO] [Epoch:48/50]: loss1:0.0052 loss2:0.3086 | AUC:0.9780 FAR:0.00026
[2023-06-26 16:10:14,719][main.py][line:74][INFO] [Epoch:49/50]: loss1:0.0053 loss2:0.3102 | AUC:0.9776 FAR:0.00026
[2023-06-26 16:10:30,256][main.py][line:74][INFO] [Epoch:50/50]: loss1:0.0054 loss2:0.3068 | AUC:0.9778 FAR:0.00026
[2023-06-26 16:10:30,266][main.py][line:80][INFO] Training completes in 13m 1s | best AUC:0.9800 FAR:0.00000

[2023-06-26 16:11:33,636][main.py][line:87][INFO] Config:{'dataset': 'shanghaiTech', 'model_name': 'SH_', 'metrics': 'AUC', 'feat_prefix': '/data/pyj/feat/SHTech-i3d', 'train_list': './list/sh/train.list', 'test_list': './list/sh/test.list', 'token_feat': './list/sh/sh-prompt.npy', 'abn_label': './list/sh/relabel.list', 'gt': './list/sh/sh-gt.npy', 'win_size': 5, 'gamma': 0.08, 'bias': 0.1, 'norm': True, 't_step': 3, 'temp': 0.2, 'lamda': 9, 'seed': 0, 'test_bs': 10, 'smooth': 'slide', 'kappa': 3, 'ckpt_path': './ckpt/SH__98.pkl', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0005, 'dropout': 0.1, 'train_bs': 128, 'max_seqlen': 200, 'max_epoch': 50, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2023-06-26 16:11:36,131][main.py][line:113][INFO] total params:1.2073M
[2023-06-26 16:11:36,132][main.py][line:120][INFO] Test Mode
[2023-06-26 16:11:36,132][main.py][line:26][INFO] loading pretrained checkpoint from ./ckpt/SH__98.pkl.
[2023-06-26 16:11:39,240][infer.py][line:47][INFO] offline AUC:0.9814 AP:0.7256 FAR:0.0000 | Complete in 0m 3s

